"""
Example CLI runner for the qdd2 pipeline.

Usage:
  python main.py --text 'íŠ¸ëŸ¼í”„ "ë² ë„¤ìˆ˜ì—˜ë¼ ìƒê³µ ì „ë©´íì‡„"' --date 2024-11-29

Flags:
  --text / --file : input text (one must be provided)
  --quote         : specific quote sentence (optional; if omitted, pass None)
  --date          : article date YYYY-MM-DD (optional)
  --top-n         : number of keywords to extract (default 15)
  --top-k         : keywords used in the final query (default 3)
  --rollcall      : rollcall.com-friendly query mode (boolean flag)
  --debug         : verbose prints from NER/keyword extraction (boolean flag)
  --search        : run web search + span matching
"""

import argparse
import logging
import sys

from qdd2.snippet_matcher import find_best_span_from_candidates_debug
from qdd2.translation import translate_ko_to_en
from qdd2.pipeline import build_queries_from_text
from qdd2.search_client import google_cse_search
from qdd2.trump_utils import detect_trump_context
from qdd2.rollcall_search import get_search_results, fetch_transcript_text
from datetime import datetime


# def run_qdd2(
#     text: str | None = None,
#     file_path: str | None = None,
#     quote: str | None = None,
#     date: str | None = None,
#     top_n: int = 15,
#     top_k: int = 3,
#     rollcall: bool = False,
#     debug: bool = False,
#     search: bool = False,
#     top_matches: int = 1,  # â˜… ì¶”ê°€
# ):
#     def get_top_k_spans(
#         quote_en: str,
#         candidates: list[dict],
#         k: int,
#         num_before: int = 1,
#         num_after: int = 1,
#         min_score: float = 0.0,
#     ):
#         results = []
#         for c in candidates:
#             span = find_best_span_from_candidates_debug(
#                 quote_en=quote_en,
#                 candidates=[c],
#                 num_before=num_before,
#                 num_after=num_after,
#                 min_score=min_score,
#             )
#             if span:
#                 results.append(span)
#
#         results = sorted(results, key=lambda x: x.get("best_score", 0), reverse=True)
#         return results[:k]
#
#     """
#     QDD2 íŒŒì´í”„ë¼ì¸ì„ Python í•¨ìˆ˜ë¡œ í˜¸ì¶œí•  ìˆ˜ ìžˆê²Œ í•œ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸.
#
#     ë°˜í™˜ê°’ ì˜ˆì‹œ:
#     {
#         "pipeline_result": {...},          # build_queries_from_text ê²°ê³¼
#         "search_items": [ {...}, ... ],    # ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ (search=Trueì¼ ë•Œë§Œ)
#         "best_span": {...} or None,        # SBERT ê¸°ë°˜ best span (search=True + í›„ë³´ ìžˆì„ ë•Œë§Œ)
#     }
#     """
#     logging.basicConfig(
#         level=logging.DEBUG if debug else logging.INFO,
#         format="[%(levelname)s] %(message)s",
#     )
#     logger = logging.getLogger("qdd2.cli")
#
#     logger.info("[Step 0] Starting QDD2 pipeline (function mode)")
#
#     # 1) í…ìŠ¤íŠ¸ ë¡œë”©
#     if text is not None:
#         loaded_text = text
#     elif file_path is not None:
#         try:
#             with open(file_path, "r", encoding="utf-8") as f:
#                 loaded_text = f.read()
#         except Exception as e:
#             raise RuntimeError(f"Failed to read file {file_path}: {e}")
#     else:
#         raise ValueError("Either `text` or `file_path` must be provided.")
#
#     logger.info("[Step 1] Loaded text (%d chars)", len(loaded_text))
#     if quote:
#         logger.info("Quote provided: %s", quote)
#     if date:
#         logger.info("Article date: %s", date)
#     logger.info(
#         "Args: top_n=%d, top_k=%d, rollcall=%s, debug=%s, search=%s",
#         top_n, top_k, rollcall, debug, search,
#     )
#
#     # 2) íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ
#     logger.info("[Step 2] Calling pipeline.build_queries_from_text()")
#     result = build_queries_from_text(
#         text=loaded_text,
#         top_n_keywords=top_n,
#         top_k_for_query=top_k,
#         quote_sentence=quote,
#         article_date=date,
#         rollcall_mode=rollcall,
#         device=0,  # CPU by default
#         debug=debug,
#     )
#     logger.info("[Step 3] Pipeline completed")
#     logger.info(
#         "Summary: entities=%d, keywords=%d, queries(ko=%s / en=%s)",
#         len(result.get("entities", [])),
#         len(result.get("keywords", [])),
#         bool(result.get("queries", {}).get("ko")),
#         bool(result.get("queries", {}).get("en")),
#     )
#     quote_text = quote or ""
#
#     # 3-A) NER ê¸°ë°˜ íŠ¸ëŸ¼í”„ ê°ì§€
#     is_trump_context = detect_trump_context(
#     article_text=loaded_text,
#     quote_text=quote,
#     pipeline_result=result,
# )
#
#     logger.info("Trump context detected: %s", is_trump_context)
#     search_items: list[dict] = []
#     best_span: dict | None = None
#     span_candidates: list[dict] = []
#
#     # ---------------------
#     # 4) ê²€ìƒ‰ + SBERT ë§¤ì¹­
#     # ---------------------
#     if search:
#         logger.info("[Step 4] Running search with generated query")
#
#         query = result["queries"].get("en") or result["queries"].get("ko")
#         if not query:
#             logger.warning("No query available to search.")
#         else:
#             # 4-A) Trump + rollcall â†’ Rollcall JSON + transcript ë³¸ë¬¸ ì‚¬ìš©
#             if is_trump_context and rollcall:
#                 logger.info("[Search] Trump context + rollcall=True â†’ using Rollcall JSON search")
#                 rollcall_links: list[str] = []
#                 try:
#                     rollcall_links = get_search_results(query, top_k=5)
#                 except Exception as e:
#                     logger.warning("Rollcall search failed, fallback to CSE: %s", e)
#
#                 logger.info("[Search] Rollcall raw links: %d", len(rollcall_links))
#
#                 # Rollcall ê²°ê³¼ë¥¼ search_itemsë¡œ ëž˜í•‘
#                 search_items = [
#                     {"link": url, "snippet": ""} for url in rollcall_links if url
#                 ]
#
#                 # Rollcallì—ì„œë„ ê²°ê³¼ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ CSE fallback
#                 if not search_items:
#                     logger.info("[Search] No rollcall results, fallback to Google CSE")
#                     data = google_cse_search(query, num=20, debug=debug)
#                     search_items = data.get("items", []) or []
#
#             # 4-B) ê·¸ ì™¸ì—ëŠ” CSEë§Œ ì‚¬ìš©
#             else:
#                 logger.info("[Search] Using Google CSE (non-Trump context or rollcall=False)")
#                 data = google_cse_search(query, num=5, debug=debug)
#                 search_items = data.get("items", []) or []
#
#         if not search_items:
#             logger.warning("No results returned from search backends.")
#         else:
#             logger.info("[Search] Rollcall/CSE items: %d", len(search_items))
#             logger.info("[Step 5] Running SBERT snippet matching on search results")
#
#             # 1) ìœ ì‚¬ë„ ê³„ì‚°ìš© ì˜ì–´ ê¸°ì¤€ ë¬¸ìž¥
#             quote_for_match_en: str | None = None
#
#             if quote_text:
#                 try:
#                     quote_for_match_en = translate_ko_to_en(quote_text)
#                 except Exception as e:
#                     logger.warning("Quote translation failed, fallback to EN query: %s", e)
#
#             if not quote_for_match_en:
#                 quote_for_match_en = result["queries"].get("en")
#
#             if quote_for_match_en:
#                 candidates: list[dict] = []
#
#                 # Trump + rollcallì¸ ê²½ìš°: transcript ë³¸ë¬¸ì„ í›„ë³´ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
#                 if is_trump_context and rollcall:
#                     for it in search_items:
#                         url = it.get("link")
#                         if not url:
#                             continue
#                         try:
#                             body = fetch_transcript_text(url)
#                         except Exception as e:
#                             logger.warning("Failed to fetch transcript text: %s", e)
#                             continue
#
#                         if not body:
#                             continue
#
#                         candidates.append(
#                             {
#                                 "url": url,
#                                 "snippet": body,
#                             }
#                         )
#                 else:
#                     # ì¼ë°˜ CSE: snippet ê¸°ë°˜ í›„ë³´
#                     for it in search_items:
#                         url = it.get("link")
#                         if not url:
#                             continue
#                         snippet = it.get("snippet", "") or ""
#                         if not snippet:
#                             continue
#                         candidates.append(
#                             {
#                                 "url": url,
#                                 "snippet": snippet,
#                             }
#                         )
#
#                 if candidates:
#                     try:
#                         top_spans = get_top_k_spans(
#                             quote_en=quote_for_match_en,
#                             candidates=candidates,
#                             k=top_matches,
#                             num_before=1,
#                             num_after=1,
#                             min_score=0.0,
#                         )
#                         best_span = top_spans[0] if top_spans else None
#                         if best_span:
#                             logger.info(
#                                 "[Step 6] Best span found: score=%.4f, url=%s",
#                                 best_span.get("best_score", -1.0),
#                                 best_span.get("url", ""),
#                             )
#                             span_candidates = best_span.get("top_k_candidates", []) or []
#                         else:
#                             logger.warning("No span passed the similarity threshold.")
#                     except Exception as e:
#                         logger.warning("SBERT snippet matching failed: %s", e)
#                 else:
#                     logger.warning("No candidate texts for similarity matching.")
#             else:
#                 logger.warning("No English text available for similarity matching.")
#
#     # 4) (ì˜µì…˜) ê²€ìƒ‰ + SBERT ê¸°ë°˜ best span
#     search_items: list[dict] = []
#     best_span: dict | None = None
#     span_candidates: list[dict] = []  # â˜… ì¶”ê°€: í›„ë³´ span ë¦¬ìŠ¤íŠ¸

    # if search:
    #     logger.info("[Step 4] Running search with generated query")
    #
    #     # ì¿¼ë¦¬ëŠ” EN â†’ KO ìš°ì„  ì‚¬ìš©
    #     query = result["queries"].get("en") or result["queries"].get("ko")
    #
    #     if not query:
    #         logger.warning("No query available to search.")
    #     else:
    #         # 4-A) íŠ¸ëŸ¼í”„ ì»¨í…ìŠ¤íŠ¸ + rollcall=True â†’ Rollcall ìš°ì„ , ì‹¤íŒ¨ ì‹œ CSE
    #         if is_trump_context and rollcall:
    #             logger.info("[Search] Trump context + rollcall=True â†’ using Rollcall Selenium search first")
    #             try:
    #                 rollcall_links = get_search_results(query, top_k=5)
    #                 logger.info("[Search] Rollcall links found: %d", len(search_items))
    #
    #             except Exception as e:
    #                 logger.warning("Rollcall search failed, fallback to CSE: %s", e)
    #                 rollcall_links = []
    #
    #             search_items = [
    #                 {"link": url, "snippet": ""}
    #                 for url in rollcall_links
    #                 if url
    #             ]
    #
    #             if not search_items:
    #                 logger.info("[Search] No rollcall results, fallback to Google CSE")
    #                 data = google_cse_search(query, num=20, debug=debug)
    #                 search_items = data.get("items", []) or []
    #
    #         # 4-B) ê·¸ ì™¸ì—ëŠ” ë¬´ì¡°ê±´ CSE ì‚¬ìš©
    #         else:
    #             logger.info("[Search] Using Google CSE (non-Trump context or rollcall=False)")
    #             data = google_cse_search(query, num=5, debug=debug)
    #             search_items = data.get("items", []) or []
    #
    #     if not search_items:
    #         logger.warning("No results returned from search backends.")
    #     else:
    #         # --- ì—¬ê¸°ì„œë¶€í„° SBERT ìœ ì‚¬ë„ ê¸°ë°˜ best span ê³„ì‚° ---
    #         logger.info("[Step 5] Running SBERT snippet matching on search results")
    #
    #         # 1) ìœ ì‚¬ë„ ê³„ì‚°ì— ì‚¬ìš©í•  ì˜ì–´ ë¬¸ìž¥ ê²°ì •
    #         quote_for_match_en: str | None = None
    #
    #         if quote_text:
    #             try:
    #                 quote_for_match_en = translate_ko_to_en(quote_text)
    #             except Exception as e:
    #                 logger.warning("Quote translation failed, fallback to EN query: %s", e)
    #
    #         if not quote_for_match_en:
    #             # fallback: EN ì¿¼ë¦¬ ìžì²´ë¥¼ ì‚¬ìš©
    #             quote_for_match_en = result["queries"].get("en")
    #
    #         if quote_for_match_en:
    #             candidates = []
    #             for it in search_items:
    #                 url = it.get("link")
    #                 if not url:
    #                     continue
    #                 snippet = it.get("snippet", "") or ""
    #                 candidates.append(
    #                     {
    #                         "url": url,
    #                         "snippet": snippet,
    #                     }
    #                 )
    #
    #             if candidates:
    #                 try:
    #                     top_spans = get_top_k_spans(
    #                         quote_en=quote_for_match_en,
    #                         candidates=candidates,
    #                         k=top_matches,
    #                         num_before=1,
    #                         num_after=1,
    #                         min_score=0.2,
    #                     )
    #                     best_span = top_spans[0] if top_spans else None
    #                     if best_span:
    #                         logger.info(
    #                             "[Step 6] Best span found: score=%.4f, url=%s",
    #                             best_span.get("best_score", -1.0),
    #                             best_span.get("url", ""),
    #                         )
    #                         # â˜… ì¶”ê°€: snippet_matcherì—ì„œ ë„£ì–´ì¤€ í›„ë³´ ë¦¬ìŠ¤íŠ¸ êº¼ë‚´ê¸°
    #                         span_candidates = best_span.get("top_k_candidates", []) or []
    #                     else:
    #                         logger.warning("No span passed the similarity threshold.")
    #                 except Exception as e:
    #                     logger.warning("SBERT snippet matching failed: %s", e)
    #         else:
    #             logger.warning("No English text available for similarity matching.")
    #

def run_qdd2(
    text: str | None = None,
    file_path: str | None = None,
    quote: str | None = None,
    date: str | None = None,
    top_n: int = 15,
    top_k: int = 3,
    rollcall: bool = False,
    debug: bool = False,
    search: bool = False,
    top_matches: int = 1,
):
    """
    QDD2 íŒŒì´í”„ë¼ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (í•¨ìˆ˜ ë²„ì „).

    ë°˜í™˜ ì˜ˆì‹œ:
    {
        "pipeline_result": {...},       # build_queries_from_text ê²°ê³¼
        "search_items": [ {...}, ... ], # ê²€ìƒ‰ ê²°ê³¼
        "best_span": {...} or None,     # SBERT ê¸°ë°˜ ìµœì¢… span
        "span_candidates": [ {...}, ... ]  # top-k í›„ë³´ë“¤ (ì„ íƒ)
    }
    """
    logging.basicConfig(
        level=logging.DEBUG if debug else logging.INFO,
        format="[%(levelname)s] %(message)s",
    )
    logger = logging.getLogger("qdd2.cli")

    logger.info("[Step 0] Starting QDD2 pipeline (function mode)")

    # 1) í…ìŠ¤íŠ¸ ë¡œë”©
    if text is not None:
        loaded_text = text
    elif file_path is not None:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                loaded_text = f.read()
        except Exception as e:
            raise RuntimeError(f"Failed to read file {file_path}: {e}")
    else:
        raise ValueError("Either `text` or `file_path` must be provided.")

    logger.info("[Step 1] Loaded text (%d chars)", len(loaded_text))
    if quote:
        logger.info("Quote provided: %s", quote)
    if date:
        logger.info("Article date: %s", date)
    logger.info(
        "Args: top_n=%d, top_k=%d, rollcall=%s, debug=%s, search=%s, top_matches=%d",
        top_n, top_k, rollcall, debug, search, top_matches,
    )

    # 2) ì¿¼ë¦¬ ë¹Œë“œ
    logger.info("[Step 2] Calling pipeline.build_queries_from_text()")
    result = build_queries_from_text(
        text=loaded_text,
        top_n_keywords=top_n,
        top_k_for_query=top_k,
        quote_sentence=quote,
        article_date=date,
        rollcall_mode=rollcall,
        device=0,  # CPU
        debug=debug,
    )
    logger.info("[Step 3] Pipeline completed")
    logger.info(
        "Summary: entities=%d, keywords=%d, queries(ko=%s / en=%s)",
        len(result.get("entities", [])),
        len(result.get("keywords", [])),
        bool(result.get("queries", {}).get("ko")),
        bool(result.get("queries", {}).get("en")),
    )

    quote_text = quote or ""

    # 3) íŠ¸ëŸ¼í”„ ì»¨í…ìŠ¤íŠ¸ ê°ì§€
    is_trump_context = detect_trump_context(
        article_text=loaded_text,
        quote_text=quote,
        pipeline_result=result,
    )
    logger.info("Trump context detected: %s", is_trump_context)

    # 4) ê²€ìƒ‰ + SBERT span
    search_items: list[dict] = []
    best_span: dict | None = None
    span_candidates: list[dict] = []

    def get_top_k_spans(
        quote_en: str,
        candidates: list[dict],
        k: int,
        num_before: int = 1,
        num_after: int = 1,
        min_score: float = 0.1,   # í•„ìš”í•˜ë©´ 0.15, 0.2 ë¡œ ì¡°ì •
    ) -> list[dict]:
        """
        snippet_matcher.find_best_span_from_candidates_debug ë¥¼ í›„ë³´ë³„ë¡œ ëŒë ¤ì„œ
        top-k span dict ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.
        """
        results: list[dict] = []
        for c in candidates:
            from qdd2.snippet_matcher import find_best_span_from_candidates_debug

            span = find_best_span_from_candidates_debug(
                quote_en=quote_en,
                candidates=[c],
                num_before=num_before,
                num_after=num_after,
                min_score=min_score,
            )
            if span:
                results.append(span)

        results = sorted(results, key=lambda x: x.get("best_score", 0.0), reverse=True)
        return results[:k]

    if search:
        logger.info("[Step 4] Running search with generated query")

        queries = result.get("queries") or {}
        query = queries.get("en") or queries.get("ko")

        if not query:
            logger.warning("No query available to search.")
        else:
            # 4-A) Trump + rollcall=True â†’ Rollcall JSON ìš°ì„ 
            if is_trump_context and rollcall:
                logger.info("[Search] Trump context + rollcall=True â†’ using Rollcall JSON search")

                rollcall_links: list[str] = []
                try:
                    rollcall_links = get_search_results(query, top_k=5)
                except Exception as e:
                    logger.warning("Rollcall search failed, fallback to CSE: %s", e)

                logger.info("[Search] Rollcall raw links: %d", len(rollcall_links))

                # ðŸ”¹ ê¸°ì‚¬ ë‚ ì§œì—ì„œ ì—°ë„ ë½‘ê¸°
                target_year = None
                if date:
                    try:
                        # ë„¤ê°€ ì´ë¯¸ ì“°ëŠ” í¬ë§·ì— ë§žê²Œë§Œ íŒŒì‹±
                        for fmt in ("%Y-%m-%d", "%Y.%m.%d", "%Y/%m/%d"):
                            try:
                                target_year = datetime.strptime(str(date).strip(), fmt).year
                                break
                            except ValueError:
                                continue
                    except Exception:
                        target_year = None
                # ðŸ”¹ ì—°ë„ê°€ ìžˆìœ¼ë©´ slugì—ì„œ ì—°ë„ë¡œ í•œ ë²ˆ í•„í„°ë§
                filtered_links: list[str] = []
                if target_year:
                    year_token = f"-{target_year}/"
                    for url in rollcall_links:
                        if year_token in url:
                            filtered_links.append(url)

                    logger.info("[Search] Rollcall links filtered by year %s: %d",
                                target_year, len(filtered_links))

                # ðŸ”¹ ì—°ë„ë¡œ í•„í„°í–ˆëŠ”ë° ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                effective_links = filtered_links if filtered_links else rollcall_links

                search_items = [{"link": url, "snippet": ""} for url in effective_links if url]

                if not search_items:
                    logger.info("[Search] No rollcall results, fallback to Google CSE")
                    data = google_cse_search(query, num=20, debug=debug)
                    search_items = data.get("items", []) or []
            else:
                # 4-B) ì¼ë°˜ CSE ê²€ìƒ‰
                logger.info("[Search] Using Google CSE (non-Trump context or rollcall=False)")
                data = google_cse_search(query, num=5, debug=debug)
                search_items = data.get("items", []) or []

        if not search_items:
            logger.warning("No results returned from search backends.")
        else:
            logger.info("[Search] Rollcall/CSE items: %d", len(search_items))
            logger.info("[Step 5] Running SBERT snippet matching on search results")

            # 1) ìœ ì‚¬ë„ ê¸°ì¤€ ë¬¸ìž¥ (ì˜ì–´)
            quote_for_match_en: str | None = None

            if quote_text:
                try:
                    quote_for_match_en = translate_ko_to_en(quote_text)
                except Exception as e:
                    logger.warning("Quote translation failed, fallback to EN query: %s", e)

            if not quote_for_match_en:
                quote_for_match_en = queries.get("en")

            if quote_for_match_en:
                candidates: list[dict] = []

                # Trump + rollcall â†’ transcript ë³¸ë¬¸
                if is_trump_context and rollcall:
                    for it in search_items:
                        url = it.get("link")
                        if not url:
                            continue
                        try:
                            body = fetch_transcript_text(url)
                        except Exception as e:
                            logger.warning("Failed to fetch transcript text: %s", e)
                            continue
                        if not body:
                            continue
                        candidates.append(
                            {
                                "url": url,
                                "snippet": body,
                            }
                        )
                else:
                    # ì¼ë°˜ CSE â†’ snippet ê¸°ë°˜
                    for it in search_items:
                        url = it.get("link")
                        if not url:
                            continue
                        snippet = it.get("snippet", "") or ""
                        if not snippet:
                            continue
                        candidates.append(
                            {
                                "url": url,
                                "snippet": snippet,
                            }
                        )

                if candidates:
                    try:
                        top_spans = get_top_k_spans(
                            quote_en=quote_for_match_en,
                            candidates=candidates,
                            k=top_matches,
                            num_before=1,
                            num_after=1,
                            min_score=0.1,
                        )
                        if top_spans:
                            best_span = top_spans[0]
                            span_candidates = top_spans  # í•„ìš”í•˜ë©´ top_k ì „ì²´ ë„˜ê¹€

                            logger.info(
                                "[Step 6] Best span found: score=%.4f, url=%s",
                                best_span.get("best_score", -1.0),
                                best_span.get("url", ""),
                            )
                        else:
                            logger.warning("No span passed the similarity threshold.")
                    except Exception as e:
                        logger.warning("SBERT snippet matching failed: %s", e)
                else:
                    logger.warning("No candidate texts for similarity matching.")
            else:
                logger.warning("No English text available for similarity matching.")

    # â˜…â˜…â˜… ë°˜ë“œì‹œ ì—¬ê¸°ì—ì„œ í•œ ë²ˆë§Œ return â˜…â˜…â˜…
    return {
        "pipeline_result": result,
        "search_items": search_items,
        "best_span": best_span,
        "span_candidates": span_candidates,
    }

    # return {
    #     "pipeline_result": result,
    #     "search_items": search_items,
    #     "best_span": best_span,
    #     "span_candidates": span_candidates,  # â˜… ì¶”ê°€
    #
    # }


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="QDD2 extraction/query test runner")
    src = parser.add_mutually_exclusive_group(required=True)
    src.add_argument("--text", type=str, help="Inline text to process")
    src.add_argument("--file", type=str, help="Path to a UTF-8 text file to process")

    parser.add_argument("--quote", type=str, default=None, help="Specific quote sentence (optional)")
    parser.add_argument("--date", type=str, default=None, help="Article date YYYY-MM-DD")
    parser.add_argument("--top-n", type=int, default=15, help="Number of keywords to extract (default: 15)")
    parser.add_argument("--top-k", type=int, default=3, help="Keywords to include in query (default: 3)")
    parser.add_argument("--rollcall", action="store_true", help="Use rollcall.com-oriented query construction")
    parser.add_argument("--debug", action="store_true", help="Verbose debug logs")
    parser.add_argument("--search",action="store_true",help="Automatically run web search (Rollcall for Trump context, otherwise Google CSE)")
    parser.add_argument("--top-matches",type=int,default=1,help="Number of top similarity spans to return (default: 1)")

    return parser.parse_args()


def load_text(args: argparse.Namespace) -> str:
    if args.text:
        return args.text
    try:
        with open(args.file, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        print(f"Failed to read file {args.file}: {e}", file=sys.stderr)
        sys.exit(1)


def main():
    args = parse_args()

    out = run_qdd2(
        text=args.text,
        file_path=args.file,
        quote=args.quote,
        date=args.date,
        top_n=args.top_n,
        top_k=args.top_k,
        rollcall=args.rollcall,
        debug=args.debug,
        search=args.search,
        top_matches=args.top_matches,   # â˜… ì¶”ê°€
    )

    result = out["pipeline_result"]
    items = out["search_items"]

    print("\n=== Entities by type ===")
    for label, words in result["entities_by_type"].items():
        print(f"{label}: {words}")

    print("\n=== Top keywords ===")
    for kw, score in result["keywords"]:
        print(f"{kw}  ({score:.4f})")

    print("\n=== Queries ===")
    print(f"KO: {result['queries']['ko']}")
    print(f"EN: {result['queries']['en']}")

    if args.search and items:
        print("\n=== Top search results ===")
        for item in items[:5]:
            print(f"- {item.get('title', '').strip()} :: {item.get('link', '')}")

    best_span = out.get("best_span")

    if args.search and out.get("top_spans"):
        print("\n=== Top SBERT similarity spans ===")
        for i, span in enumerate(out["top_spans"], 1):
            print(f"\n# {i}")
            print(f"URL        : {span.get('url', '')}")
            print(f"SCORE      : {span.get('best_score', -1.0):.4f}")
            print(f"SENTENCE   : {span.get('best_sentence', '')}")
            print(f"SPAN TEXT  : {span.get('span_text', '')}")


if __name__ == "__main__":
    main()
